{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhahbimohamed/PC-Parts-Recommender-System/blob/main/Notebooks/01_Scrape_MohamedDhahbi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scrape Data**"
      ],
      "metadata": {
        "id": "cgR5kpIaaVrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Install what i need**"
      ],
      "metadata": {
        "id": "IVZ5tsNwarF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4 pandas lxml fake_useragent\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c6WMtJSdINYk",
        "outputId": "f5fe8972-6d9f-4d13-e761-a8c5253bf3c2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "da84f3f6",
        "outputId": "155997b6-cedc-41dd-df71-c9120312e017"
      },
      "source": [
        "!playwright install"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:934:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1056:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1045:7)\n",
            "    at async i.<anonymous> (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/cli/program.js:217:7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CPUs**"
      ],
      "metadata": {
        "id": "D0u2ObYWa9L8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm.asyncio import tqdm\n",
        "import time\n",
        "\n",
        "# SPEC EXTRACTOR\n",
        "def extract_specs(text):\n",
        "    text = text.replace(\",\", \".\").replace(\"-\", \" \")\n",
        "    specs = {}\n",
        "\n",
        "    # Cores\n",
        "    match = re.search(r'(\\d+)\\s*(?:core|coeur|coeurs|cores?|C)\\b', text, re.I)\n",
        "    specs[\"Cores\"] = match.group(1) if match else None\n",
        "\n",
        "    # Threads\n",
        "    match = re.search(r'(\\d+)\\s*(?:thread|threads|T)\\b', text, re.I)\n",
        "    specs[\"Threads\"] = match.group(1) if match else None\n",
        "\n",
        "    # Frequency (GHz)\n",
        "    match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(?:GHz|GHZ)', text, re.I)\n",
        "    specs[\"Frequency (GHz)\"] = match.group(1) if match else None\n",
        "\n",
        "    # Cache (MB)\n",
        "    match = re.search(r'(\\d+)\\s*(?:MB|Mo)\\s*(?:cache|L\\d)?', text, re.I)\n",
        "    specs[\"Cache (MB)\"] = match.group(1) if match else None\n",
        "\n",
        "    # Socket\n",
        "    match = re.search(r'(AM\\d+|LGA\\s*\\d+)', text, re.I)\n",
        "    specs[\"Socket\"] = match.group(1).replace(\" \", \"\") if match else None\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "async def scrape_amazon_cpus():\n",
        "    start_url = \"https://www.amazon.fr/-/en/gp/browse.html?rw_useCurrentProtocol=1&node=430352031&ref_=amb_link_sj7jvnB9SQalv7yCEAD-TA_28\"\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        current_url = start_url\n",
        "        while True:\n",
        "            print(f\"\\n Loading page {page_number}: {current_url}\")\n",
        "            await page.goto(current_url)\n",
        "            try:\n",
        "                await page.wait_for_selector(\"div[data-cy='title-recipe']\", timeout=25000)\n",
        "            except:\n",
        "                print(\"No products found. Stopping.\")\n",
        "                break\n",
        "\n",
        "            html = await page.content()\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            products = soup.select(\"div[data-cy='title-recipe']\")\n",
        "\n",
        "            for product in tqdm(products, desc=f\"Extracting page {page_number}\"):\n",
        "                link_tag = product.select_one(\"a.a-link-normal\")\n",
        "                title_tag = product.select_one(\"a h2 span\")\n",
        "\n",
        "                if not title_tag or not link_tag:\n",
        "                    continue\n",
        "\n",
        "                name = title_tag.get_text(strip=True)\n",
        "                link = \"https://www.amazon.fr\" + link_tag[\"href\"]\n",
        "\n",
        "                # Try to find a secondary line (subtitle/specs)\n",
        "                sub_text = \"\"\n",
        "                subtitle_tag = product.find_next(\"div\", attrs={\"data-cy\": \"secondary-description-recipe\"})\n",
        "                if subtitle_tag:\n",
        "                    sub_text = subtitle_tag.get_text(strip=True)\n",
        "\n",
        "                full_text = f\"{name} {sub_text}\"\n",
        "\n",
        "                # Find price in same container\n",
        "                container = product.find_parent(\"div\", class_=\"puisg-col-inner\")\n",
        "                price_tag = None\n",
        "                if container:\n",
        "                    price_tag = container.select_one(\"div[data-cy='price-recipe'] span.a-offscreen\")\n",
        "                price = price_tag.get_text(strip=True) if price_tag else \"N/A\"\n",
        "\n",
        "                specs = extract_specs(full_text)\n",
        "                all_data.append({\n",
        "                    \"Name\": name,\n",
        "                    \"URL\": link,\n",
        "                    \"Price\": price,\n",
        "                    **specs\n",
        "                })\n",
        "\n",
        "            # Pagination\n",
        "            next_button = soup.select_one(\"a.s-pagination-next\")\n",
        "            if next_button and \"href\" in next_button.attrs and \"disabled\" not in next_button.get(\"class\", []):\n",
        "                next_href = next_button[\"href\"]\n",
        "                current_url = \"https://www.amazon.fr\" + next_href\n",
        "                page_number += 1\n",
        "                await asyncio.sleep(2)\n",
        "            else:\n",
        "                print(\"No more pages found.\")\n",
        "                break\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    # Clean and export\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.drop_duplicates(subset=[\"Name\"], inplace=True)\n",
        "    df.to_csv(\"amazon_cpus.csv\", index=False)\n",
        "    print(f\"\\nScraping complete! {len(df)} CPUs saved to amazon_cpus.csv\")\n",
        "\n",
        "\n",
        "await scrape_amazon_cpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "id": "-Cdm0bMxXm-2",
        "outputId": "d4c974f2-a56b-47c9-e07d-4873263a96b9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2613128535.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mscrape_amazon_cpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2613128535.py\u001b[0m in \u001b[0;36mscrape_amazon_cpus\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mpage_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0masync_playwright\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchromium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheadless\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/playwright/async_api/_context_manager.py\u001b[0m in \u001b[0;36m__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mplaywright_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaywright_future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         done, _ = await asyncio.wait(\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_error_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaywright_future\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mreturn_when\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFIRST_COMPLETED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(fs, timeout, return_when)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_when\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(fs, timeout, return_when, loop)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0;32mawait\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout_handle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPUs**"
      ],
      "metadata": {
        "id": "RWiSqjD2fItg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re # Import the re module for regex\n",
        "from tqdm.asyncio import tqdm\n",
        "import time\n",
        "\n",
        "# === SMART SPEC EXTRACTOR ===\n",
        "def extract_specs(text):\n",
        "    text = text.replace(\",\", \".\").replace(\"-\", \" \")\n",
        "    specs = {}\n",
        "\n",
        "    # Cores\n",
        "    match = re.search(r'(\\d+)\\s*(?:core|coeur|coeurs|cores?|C)\\b', text, re.I)\n",
        "    specs[\"Cores\"] = match.group(1) if match else None\n",
        "\n",
        "    # Threads\n",
        "    match = re.search(r'(\\d+)\\s*(?:thread|threads|T)\\b', text, re.I)\n",
        "    specs[\"Threads\"] = match.group(1) if match else None\n",
        "\n",
        "    # Frequency (GHz)\n",
        "    match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(?:GHz|GHZ)', text, re.I)\n",
        "    specs[\"Frequency (GHz)\"] = match.group(1) if match else None\n",
        "\n",
        "    # Cache (MB)\n",
        "    match = re.search(r'(\\d+)\\s*(?:MB|Mo)\\s*(?:cache|L\\d)?', text, re.I)\n",
        "    specs[\"Cache (MB)\"] = match.group(1) if match else None\n",
        "\n",
        "    # Socket\n",
        "    match = re.search(r'(AM\\d+|LGA\\s*\\d+)', text, re.I)\n",
        "    specs[\"Socket\"] = match.group(1).replace(\" \", \"\") if match else None\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "async def scrape_amazon_gpus():\n",
        "    start_url = \"https://www.amazon.fr/-/en/gp/browse.html?rw_useCurrentProtocol=1&node=430340031&ref_=amb_link_sj7jvnB9SQalv7yCEAD-TA_29\"\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        current_url = start_url\n",
        "        while True:\n",
        "            print(f\"\\n Loading page {page_number}: {current_url}\")\n",
        "            await page.goto(current_url)\n",
        "            try:\n",
        "                await page.wait_for_selector(\"div[data-cy='title-recipe']\", timeout=25000)\n",
        "            except:\n",
        "                print(\" No products found. Stopping.\")\n",
        "                break\n",
        "\n",
        "            html = await page.content()\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            products = soup.select(\"div[data-cy='title-recipe']\")\n",
        "\n",
        "            for product in tqdm(products, desc=f\"Extracting GPUs page {page_number}\"):\n",
        "                # Product Title & URL\n",
        "                link_tag = product.select_one(\"a.a-link-normal\")\n",
        "                title_tag = product.select_one(\"a h2 span\")\n",
        "                if not title_tag or not link_tag:\n",
        "                    continue\n",
        "\n",
        "                name = title_tag.get_text(strip=True)\n",
        "                link = \"https://www.amazon.fr\" + link_tag[\"href\"]\n",
        "\n",
        "                # Find parent container\n",
        "                container = product.find_parent(\"div\", class_=\"puisg-col-inner\")\n",
        "                if not container:\n",
        "                    continue\n",
        "\n",
        "                # Price\n",
        "                price_tag = container.select_one(\"div[data-cy='price-recipe'] span.a-offscreen\")\n",
        "                price = price_tag.get_text(strip=True) if price_tag else \"N/A\"\n",
        "\n",
        "                # Specs block\n",
        "                specs_block = container.select_one(\"div[data-cy='product-details-recipe']\")\n",
        "                specs = {}\n",
        "                if specs_block:\n",
        "                    for spec in specs_block.select(\"div.puisg-col-inner\"):\n",
        "                        label_tag = spec.select_one(\"span.a-color-secondary\")\n",
        "                        value_tag = spec.select_one(\"span.a-text-bold\")\n",
        "                        if label_tag and value_tag:\n",
        "                            key = label_tag.get_text(strip=True)\n",
        "                            val = value_tag.get_text(strip=True)\n",
        "                            specs[key] = val\n",
        "\n",
        "                # Merge all data\n",
        "                data = {\n",
        "                    \"Name\": name,\n",
        "                    \"URL\": link,\n",
        "                    \"Price\": price,\n",
        "                }\n",
        "                data.update(specs)\n",
        "                all_data.append(data)\n",
        "\n",
        "            # Pagination handling\n",
        "            next_button = soup.select_one(\"a.s-pagination-next\")\n",
        "            if next_button and \"href\" in next_button.attrs and \"disabled\" not in next_button.get(\"class\", []):\n",
        "                next_href = next_button[\"href\"]\n",
        "                current_url = \"https://www.amazon.fr\" + next_href\n",
        "                page_number += 1\n",
        "                await asyncio.sleep(2) # small delay to avoid being blocked\n",
        "            else:\n",
        "                print(\" No more pages found.\")\n",
        "                break\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    # Export to CSV\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.drop_duplicates(subset=[\"Name\"], inplace=True)\n",
        "    df.to_csv(\"amazon_gpus.csv\", index=False)\n",
        "    print(f\"\\n Scraping complete! {len(df)} graphics cards saved to amazon_gpus.csv\")\n",
        "\n",
        "# Run\n",
        "await scrape_amazon_gpus()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dOmslq6gY3ZA",
        "outputId": "00b87d52-6c06-46fd-9a04-7d749c03fd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 1: https://www.amazon.fr/-/en/gp/browse.html?rw_useCurrentProtocol=1&node=430340031&ref_=amb_link_sj7jvnB9SQalv7yCEAD-TA_29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 1: 100%|██████████| 24/24 [00:00<00:00, 769.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 2: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=2&xpid=-8Th6JWl_oz2l&c=ts&qid=1760138436&ts_id=430340031&ref=sr_pg_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 2: 100%|██████████| 24/24 [00:00<00:00, 433.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 3: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=3&c=ts&qid=1760138443&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 3: 100%|██████████| 24/24 [00:00<00:00, 787.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 4: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=4&c=ts&qid=1760138449&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 4: 100%|██████████| 24/24 [00:00<00:00, 447.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 5: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=5&c=ts&qid=1760138455&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 5: 100%|██████████| 24/24 [00:00<00:00, 490.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 6: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=6&c=ts&qid=1760138461&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 6: 100%|██████████| 24/24 [00:00<00:00, 774.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 7: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=7&c=ts&qid=1760138467&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 7: 100%|██████████| 24/24 [00:00<00:00, 604.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 8: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=8&c=ts&qid=1760138472&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 8: 100%|██████████| 24/24 [00:00<00:00, 419.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 9: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=9&c=ts&qid=1760138477&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 9: 100%|██████████| 24/24 [00:00<00:00, 739.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 10: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=10&c=ts&qid=1760138483&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 10: 100%|██████████| 24/24 [00:00<00:00, 704.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 11: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=11&c=ts&qid=1760138488&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 11: 100%|██████████| 24/24 [00:00<00:00, 444.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 12: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=12&c=ts&qid=1760138493&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 12: 100%|██████████| 24/24 [00:00<00:00, 1039.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 13: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=13&c=ts&qid=1760138499&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 13: 100%|██████████| 24/24 [00:00<00:00, 994.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 14: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=14&c=ts&qid=1760138504&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 14: 100%|██████████| 24/24 [00:00<00:00, 487.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 15: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=15&c=ts&qid=1760138509&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 15: 100%|██████████| 24/24 [00:00<00:00, 934.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 16: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=16&c=ts&qid=1760138514&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 16: 100%|██████████| 24/24 [00:00<00:00, 814.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 17: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=17&c=ts&qid=1760138519&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 17: 100%|██████████| 24/24 [00:00<00:00, 841.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 18: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=18&c=ts&qid=1760138523&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 18: 100%|██████████| 24/24 [00:00<00:00, 526.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 19: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=19&c=ts&qid=1760138528&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 19: 100%|██████████| 24/24 [00:00<00:00, 901.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 20: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=20&c=ts&qid=1760138534&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 20: 100%|██████████| 24/24 [00:00<00:00, 922.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 21: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=21&c=ts&qid=1760138538&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 21: 100%|██████████| 24/24 [00:00<00:00, 445.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 22: https://www.amazon.fr/s?k=Graphics+Cards&i=computers&rh=n%3A430340031&page=22&c=ts&qid=1760138543&ts_id=430340031&xpid=-8Th6JWl_oz2l&ref=sr_pg_21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting GPUs page 22: 100%|██████████| 15/15 [00:00<00:00, 508.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ No more pages found.\n",
            "\n",
            "✅ Scraping complete! 493 graphics cards saved to amazon_gpus.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Motherboards**"
      ],
      "metadata": {
        "id": "b3a-ZcAEbEZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm.asyncio import tqdm # Use async tqdm\n",
        "import time\n",
        "\n",
        "def extract_mobo_specs(title: str):\n",
        "    \"\"\"Extract motherboard specs from Amazon title with more precision.\"\"\"\n",
        "    specs = {}\n",
        "    t = title.replace(\",\", \" \").replace(\"/\", \" \").replace(\"-\", \" \").lower()\n",
        "\n",
        "    # Brand\n",
        "    m = re.search(r'\\b(asus|msi|gigabyte|asrock|biostar|nzxt)\\b', t)\n",
        "    specs[\"Brand\"] = m.group(0).capitalize() if m else None\n",
        "\n",
        "    # Chipset (X870E, B650, Z790, H670, etc.)\n",
        "    m = re.search(r'\\b([bxzh]?\\d{3,4}[a-z]?)\\b', t)\n",
        "    specs[\"Chipset\"] = m.group(0).upper() if m else None\n",
        "\n",
        "    # CPU Socket\n",
        "    m = re.search(r'\\b(am\\d|lga\\d+)\\b', t)\n",
        "    specs[\"Socket\"] = m.group(0).upper() if m else None\n",
        "\n",
        "    # Form Factor\n",
        "    m = re.search(r'\\b(micro ?atx|mini ?itx|atx|e ?atx)\\b', t)\n",
        "    specs[\"Form Factor\"] = m.group(0).replace(\" \", \"\").upper() if m else None\n",
        "\n",
        "    # Memory Type + optional OC speed\n",
        "    mem_match = re.search(r'\\b(ddr4|ddr5)(?:\\s*[\\w+-]*\\s*(\\d{3,5})?mt/s)?', t)\n",
        "    if mem_match:\n",
        "        specs[\"Memory Type\"] = mem_match.group(1).upper()\n",
        "        specs[\"Memory Max OC\"] = mem_match.group(2) if mem_match.group(2) else None\n",
        "    else:\n",
        "        specs[\"Memory Type\"] = None\n",
        "        specs[\"Memory Max OC\"] = None\n",
        "\n",
        "    # Wi-Fi standard\n",
        "    wifi_match = re.search(r'wifi\\s*([678]?e?)', t)\n",
        "    specs[\"WiFi\"] = \"Yes\" if wifi_match else \"No\"\n",
        "    specs[\"WiFi Standard\"] = \"WiFi \" + wifi_match.group(1) if wifi_match else None\n",
        "\n",
        "    # PCIe Gen (handle multiple slots)\n",
        "    pcie_match = re.findall(r'pcie\\s*([0-9](?:\\.[0-9])?)', t)\n",
        "    specs[\"PCIe\"] = \", \".join(pcie_match) if pcie_match else None\n",
        "\n",
        "    # M.2 support / Gen\n",
        "    m2_match = re.findall(r'm\\.?2\\s*gen\\s*([0-9])', t)\n",
        "    specs[\"M2 Gen\"] = \", \".join(m2_match) if m2_match else None\n",
        "\n",
        "    # VRM / other optional info (like 110A)\n",
        "    vrm_match = re.search(r'vrm\\s*(\\d{2,3}a?)', t)\n",
        "    specs[\"VRM\"] = vrm_match.group(1).upper() if vrm_match else None\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "async def scrape_motherboards(max_pages=75): # Adjusted max_pages to match the call\n",
        "    start_url = \"https://www.amazon.fr/-/en/gp/browse.html?rw_useCurrentProtocol=1&node=430341031&ref_=amb_link_sj7jvnB9SQalv7yCEAD-TA_31\"\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        current_url = start_url\n",
        "\n",
        "        while page_number <= max_pages:\n",
        "            print(f\"\\n🕐 Loading page {page_number}: {current_url}\")\n",
        "            try:\n",
        "                await page.goto(current_url)\n",
        "            except Exception as e:\n",
        "                print(f\" Failed to load page {page_number}: {e}\")\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                await page.wait_for_selector(\"div[data-cy='title-recipe']\", timeout=20000)\n",
        "            except PlaywrightTimeoutError:\n",
        "                print(\" Timeout waiting for product titles. Stopping.\")\n",
        "                break\n",
        "\n",
        "            html = await page.content()\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            products = soup.select(\"div[data-cy='title-recipe']\")\n",
        "\n",
        "            if not products:\n",
        "                print(\" No products found on this page. Possibly end.\")\n",
        "                break\n",
        "\n",
        "            for product in tqdm(products, desc=f\"Extracting page {page_number}\"):\n",
        "                link_tag = product.select_one(\"a.a-link-normal\")\n",
        "                title_tag = product.select_one(\"a h2 span\")\n",
        "                if not link_tag or not title_tag:\n",
        "                    continue\n",
        "\n",
        "                name = title_tag.get_text(strip=True)\n",
        "                link = \"https://www.amazon.fr\" + link_tag.get(\"href\", \"\")\n",
        "\n",
        "                # Price\n",
        "                container = product.find_parent(\"div\", class_=\"puisg-col-inner\")\n",
        "                price = \"N/A\"\n",
        "                if container:\n",
        "                    price_tag = container.select_one(\"div[data-cy='price-recipe'] span.a-offscreen\")\n",
        "                    if price_tag:\n",
        "                        price = price_tag.get_text(strip=True)\n",
        "\n",
        "                specs = extract_mobo_specs(name)\n",
        "\n",
        "                row = {\n",
        "                    \"Name\": name,\n",
        "                    \"URL\": link,\n",
        "                    \"Price\": price,\n",
        "                    **specs\n",
        "                }\n",
        "                all_data.append(row)\n",
        "\n",
        "            # Pagination – find “Next” button\n",
        "            next_btn = soup.select_one(\"a.s-pagination-next\")\n",
        "            if next_btn and next_btn.has_attr(\"href\") and \"disabled\" not in next_btn.get(\"class\", []):\n",
        "                next_href = next_btn[\"href\"]\n",
        "                current_url = \"https://www.amazon.fr\" + next_href\n",
        "                page_number += 1\n",
        "                await asyncio.sleep(1 + (page_number % 2))  # small variable delay, use await\n",
        "            else:\n",
        "                print(\"No next page found or disabled. Done.\")\n",
        "                break\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.drop_duplicates(subset=[\"Name\"], inplace=True)\n",
        "    out_file = \"amazon_motherboards_all.csv\"\n",
        "    df.to_csv(out_file, index=False)\n",
        "    print(f\"\\n Scraping done. {len(df)} motherboards saved to {out_file}\")\n",
        "\n",
        "# Run the scraper\n",
        "await scrape_motherboards(max_pages=75) # Use await"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3WqV29U7jd_e",
        "outputId": "711d6a44-1b83-47b2-a634-89f337c20570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 1: https://www.amazon.fr/-/en/gp/browse.html?rw_useCurrentProtocol=1&node=430341031&ref_=amb_link_sj7jvnB9SQalv7yCEAD-TA_31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 1: 100%|██████████| 24/24 [00:00<00:00, 2566.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 2: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=2&xpid=ZI83ORZ8tsFrB&c=ts&qid=1760141076&ts_id=430341031&ref=sr_pg_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 2: 100%|██████████| 24/24 [00:00<00:00, 289.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 3: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=3&c=ts&qid=1760141081&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 3: 100%|██████████| 24/24 [00:00<00:00, 2646.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 4: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=4&c=ts&qid=1760141087&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 4: 100%|██████████| 24/24 [00:00<00:00, 2568.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 5: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=5&c=ts&qid=1760141092&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 5: 100%|██████████| 24/24 [00:00<00:00, 3009.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 6: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=6&c=ts&qid=1760141097&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 6: 100%|██████████| 24/24 [00:00<00:00, 1485.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 7: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=7&c=ts&qid=1760141101&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 7: 100%|██████████| 24/24 [00:00<00:00, 1973.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 8: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=8&c=ts&qid=1760141106&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 8: 100%|██████████| 30/30 [00:00<00:00, 1885.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 9: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=9&c=ts&qid=1760141110&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 9: 100%|██████████| 24/24 [00:00<00:00, 2093.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 10: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=10&c=ts&qid=1760141114&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 10: 100%|██████████| 30/30 [00:00<00:00, 940.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 11: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=11&c=ts&qid=1760141118&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 11: 100%|██████████| 24/24 [00:00<00:00, 2825.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 12: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=12&c=ts&qid=1760141124&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 12: 100%|██████████| 24/24 [00:00<00:00, 2461.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 13: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=13&c=ts&qid=1760141128&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 13: 100%|██████████| 24/24 [00:00<00:00, 2417.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 14: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=14&c=ts&qid=1760141132&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 14: 100%|██████████| 30/30 [00:00<00:00, 1300.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 15: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=15&c=ts&qid=1760141136&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 15: 100%|██████████| 24/24 [00:00<00:00, 1422.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 16: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=16&c=ts&qid=1760141141&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 16: 100%|██████████| 24/24 [00:00<00:00, 1445.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 17: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=17&c=ts&qid=1760141145&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 17: 100%|██████████| 24/24 [00:00<00:00, 1407.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 18: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=18&c=ts&qid=1760141150&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 18: 100%|██████████| 24/24 [00:00<00:00, 890.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 19: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=19&c=ts&qid=1760141154&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_18\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 19: 100%|██████████| 24/24 [00:00<00:00, 2279.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 20: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=20&c=ts&qid=1760141160&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 20: 100%|██████████| 24/24 [00:00<00:00, 1973.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 21: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=21&c=ts&qid=1760141163&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 21: 100%|██████████| 24/24 [00:00<00:00, 1638.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 22: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=22&c=ts&qid=1760141168&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_21\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 22: 100%|██████████| 30/30 [00:00<00:00, 1223.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 23: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=23&c=ts&qid=1760141171&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 23: 100%|██████████| 24/24 [00:00<00:00, 1936.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 24: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=24&c=ts&qid=1760141177&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 24: 100%|██████████| 24/24 [00:00<00:00, 1475.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 25: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=25&c=ts&qid=1760141180&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 25: 100%|██████████| 30/30 [00:00<00:00, 1377.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 26: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=26&c=ts&qid=1760141184&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 26: 100%|██████████| 24/24 [00:00<00:00, 1454.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 27: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=27&c=ts&qid=1760141188&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 27: 100%|██████████| 24/24 [00:00<00:00, 2543.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 28: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=28&c=ts&qid=1760141193&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 28: 100%|██████████| 24/24 [00:00<00:00, 2636.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 29: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=29&c=ts&qid=1760141197&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 29: 100%|██████████| 24/24 [00:00<00:00, 1411.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 30: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=30&c=ts&qid=1760141201&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 30: 100%|██████████| 24/24 [00:00<00:00, 1274.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 31: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=31&c=ts&qid=1760141205&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 31: 100%|██████████| 24/24 [00:00<00:00, 2183.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 32: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=32&c=ts&qid=1760141211&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_31\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 32: 100%|██████████| 24/24 [00:00<00:00, 1536.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 33: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=33&c=ts&qid=1760141214&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 33: 100%|██████████| 24/24 [00:00<00:00, 1764.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 34: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=34&c=ts&qid=1760141218&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_33\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 34: 100%|██████████| 24/24 [00:00<00:00, 1341.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 35: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=35&c=ts&qid=1760141222&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 35: 100%|██████████| 24/24 [00:00<00:00, 2533.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 36: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=36&c=ts&qid=1760141227&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 36: 100%|██████████| 24/24 [00:00<00:00, 2386.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 37: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=37&c=ts&qid=1760141231&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_36\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 37: 100%|██████████| 24/24 [00:00<00:00, 1988.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 38: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=38&c=ts&qid=1760141235&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_37\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 38: 100%|██████████| 24/24 [00:00<00:00, 1537.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 39: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=39&c=ts&qid=1760141239&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_38\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 39: 100%|██████████| 24/24 [00:00<00:00, 2678.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 40: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=40&c=ts&qid=1760141244&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_39\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 40: 100%|██████████| 24/24 [00:00<00:00, 2301.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 41: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=41&c=ts&qid=1760141247&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_40\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 41: 100%|██████████| 24/24 [00:00<00:00, 1619.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 42: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=42&c=ts&qid=1760141252&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_41\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 42: 100%|██████████| 24/24 [00:00<00:00, 1427.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 43: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=43&c=ts&qid=1760141256&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 43: 100%|██████████| 24/24 [00:00<00:00, 2226.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 44: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=44&c=ts&qid=1760141261&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_43\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 44: 100%|██████████| 24/24 [00:00<00:00, 3001.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 45: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=45&c=ts&qid=1760141264&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 45: 100%|██████████| 24/24 [00:00<00:00, 1267.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 46: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=46&c=ts&qid=1760141268&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 46: 100%|██████████| 24/24 [00:00<00:00, 1315.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 47: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=47&c=ts&qid=1760141272&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_46\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 47: 100%|██████████| 24/24 [00:00<00:00, 2398.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 48: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=48&c=ts&qid=1760141277&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_47\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 48: 100%|██████████| 24/24 [00:00<00:00, 2913.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 49: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=49&c=ts&qid=1760141281&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_48\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 49: 100%|██████████| 24/24 [00:00<00:00, 1606.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 50: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=50&c=ts&qid=1760141285&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_49\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 50: 100%|██████████| 24/24 [00:00<00:00, 1236.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 51: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=51&c=ts&qid=1760141290&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 51: 100%|██████████| 24/24 [00:00<00:00, 2236.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 52: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=52&c=ts&qid=1760141295&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_51\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 52: 100%|██████████| 24/24 [00:00<00:00, 1571.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 53: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=53&c=ts&qid=1760141299&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 53: 100%|██████████| 24/24 [00:00<00:00, 1228.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 54: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=54&c=ts&qid=1760141303&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_53\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 54: 100%|██████████| 24/24 [00:00<00:00, 2299.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 55: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=55&c=ts&qid=1760141308&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_54\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 55: 100%|██████████| 24/24 [00:00<00:00, 2347.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 56: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=56&c=ts&qid=1760141313&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 56: 100%|██████████| 24/24 [00:00<00:00, 2184.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 57: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=57&c=ts&qid=1760141316&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 57: 100%|██████████| 24/24 [00:00<00:00, 1512.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 58: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=58&c=ts&qid=1760141321&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_57\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 58: 100%|██████████| 24/24 [00:00<00:00, 3226.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 59: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=59&c=ts&qid=1760141325&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 59: 100%|██████████| 24/24 [00:00<00:00, 2454.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 60: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=60&c=ts&qid=1760141329&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_59\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 60: 100%|██████████| 24/24 [00:00<00:00, 2668.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 61: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=61&c=ts&qid=1760141332&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_60\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 61: 100%|██████████| 24/24 [00:00<00:00, 1421.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 62: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=62&c=ts&qid=1760141337&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_61\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 62: 100%|██████████| 24/24 [00:00<00:00, 2514.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 63: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=63&c=ts&qid=1760141341&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_62\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 63: 100%|██████████| 24/24 [00:00<00:00, 2577.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 64: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=64&c=ts&qid=1760141345&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 64: 100%|██████████| 24/24 [00:00<00:00, 1447.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 65: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=65&c=ts&qid=1760141348&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 65: 100%|██████████| 24/24 [00:00<00:00, 1085.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 66: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=66&c=ts&qid=1760141353&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 66: 100%|██████████| 24/24 [00:00<00:00, 2581.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 67: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=67&c=ts&qid=1760141357&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_66\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 67: 100%|██████████| 24/24 [00:00<00:00, 2330.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 68: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=68&c=ts&qid=1760141362&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_67\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 68: 100%|██████████| 24/24 [00:00<00:00, 1168.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 69: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=69&c=ts&qid=1760141365&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_68\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 69: 100%|██████████| 24/24 [00:00<00:00, 1876.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 70: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=70&c=ts&qid=1760141369&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_69\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 70: 100%|██████████| 24/24 [00:00<00:00, 1923.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 71: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=71&c=ts&qid=1760141373&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_70\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 71: 100%|██████████| 24/24 [00:00<00:00, 2933.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 72: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=72&c=ts&qid=1760141377&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_71\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 72: 100%|██████████| 24/24 [00:00<00:00, 1616.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 73: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=73&c=ts&qid=1760141380&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_72\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 73: 100%|██████████| 24/24 [00:00<00:00, 1202.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 74: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=74&c=ts&qid=1760141385&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_73\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 74: 100%|██████████| 24/24 [00:00<00:00, 1943.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🕐 Loading page 75: https://www.amazon.fr/s?k=Motherboards&i=computers&rh=n%3A430341031&page=75&c=ts&qid=1760141389&ts_id=430341031&xpid=ZI83ORZ8tsFrB&ref=sr_pg_74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 75: 100%|██████████| 24/24 [00:00<00:00, 2664.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Scraping done. 1698 motherboards saved to amazon_motherboards_all.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RAM**"
      ],
      "metadata": {
        "id": "LSyg8BBznfPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "# --- Function to extract RAM specs from product title ---\n",
        "def extract_ram_specs(title: str):\n",
        "    specs = {}\n",
        "    t = title.lower()\n",
        "\n",
        "    # Brand\n",
        "    m = re.search(r'\\b(crucial|corsair|g\\.skill|kingston|patriot|teamgroup)\\b', t)\n",
        "    specs[\"Brand\"] = m.group(0).capitalize() if m else None\n",
        "\n",
        "    # Model\n",
        "    m = re.search(r'\\b([a-z0-9]+)\\s+pro\\s+ram\\b', t) # This regex might need refinement based on actual titles\n",
        "    specs[\"Model\"] = m.group(1).upper() if m else None\n",
        "\n",
        "    # Total Capacity\n",
        "    m = re.search(r'(\\d{2,4})gb', t)\n",
        "    specs[\"Total Capacity\"] = m.group(1) + \"GB\" if m else None\n",
        "\n",
        "    # Kit / Module\n",
        "    m = re.search(r'\\((\\d+x\\d+gb)\\)', t)\n",
        "    specs[\"Kit\"] = m.group(1) if m else None\n",
        "\n",
        "    # Speed\n",
        "    m = re.search(r'(\\d{3,5})mhz', t)\n",
        "    specs[\"Speed\"] = m.group(1) + \"MHz\" if m else None\n",
        "\n",
        "    # CAS Latency\n",
        "    m = re.search(r'cl(\\d+)', t)\n",
        "    specs[\"CAS Latency\"] = \"CL\" + m.group(1) if m else None\n",
        "\n",
        "    # Type\n",
        "    if \"ddr4\" in t:\n",
        "        specs[\"Type\"] = \"DDR4\"\n",
        "    elif \"ddr5\" in t or (specs.get(\"Speed\") and int(specs[\"Speed\"].replace(\"MHz\",\"\")) >= 4800):\n",
        "        specs[\"Type\"] = \"DDR5\"\n",
        "    else:\n",
        "        specs[\"Type\"] = None\n",
        "\n",
        "    # Features\n",
        "    features = []\n",
        "    if \"xmp\" in t:\n",
        "        features.append(\"Intel XMP\")\n",
        "    if \"expo\" in t:\n",
        "        features.append(\"AMD EXPO\")\n",
        "    if \"overclock\" in t:\n",
        "        features.append(\"Overclocking\")\n",
        "    specs[\"Features\"] = \", \".join(features) if features else None\n",
        "\n",
        "    return specs\n",
        "\n",
        "# --- Function to parse products from one page ---\n",
        "async def parse_amazon_page(page):\n",
        "    products = []\n",
        "    # select product blocks - using a more specific selector\n",
        "    divs = await page.query_selector_all(\"div[data-cy='title-recipe']\")\n",
        "    for div in divs:\n",
        "        title_el = await div.query_selector(\"h2\")\n",
        "        # Find parent container to locate price, rating, reviews\n",
        "        container = await div.query_selector(\"xpath=..\") # Find parent div\n",
        "        price_el = await container.query_selector(\"span.a-price\") if container else None\n",
        "        rating_el = await container.query_selector(\"i.a-icon-star-mini\") if container else None\n",
        "        reviews_el = await container.query_selector(\"a[href*='customerReviews']\") if container else None\n",
        "\n",
        "\n",
        "        if title_el:\n",
        "            title = await title_el.inner_text()\n",
        "            specs = extract_ram_specs(title) # This function is not async\n",
        "            specs[\"Title\"] = title\n",
        "            specs[\"Price\"] = await price_el.inner_text() if price_el else None\n",
        "            specs[\"Rating\"] = await rating_el.get_attribute(\"aria-label\") if rating_el else None\n",
        "            specs[\"Reviews\"] = await reviews_el.inner_text() if reviews_el else None\n",
        "            products.append(specs)\n",
        "    return products\n",
        "\n",
        "# --- Main scraping loop ---\n",
        "async def scrape_amazon_ram(max_pages=75):\n",
        "    all_products = []\n",
        "    base_url = \"https://www.amazon.fr/-/en/gp/browse.html?rw_useCurrentProtocol=1&node=430351031&page={page}\"\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True) # headless=True for no browser window\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        for page_num in range(1, max_pages + 1):\n",
        "            print(f\"Scraping page {page_num}...\")\n",
        "            url = base_url.format(page=page_num)\n",
        "            try:\n",
        "                await page.goto(url, timeout=60000)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading page {page_num}: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Wait for product titles to load\n",
        "            try:\n",
        "                 await page.wait_for_selector(\"div[data-cy='title-recipe']\", timeout=20000)\n",
        "            except Exception as e:\n",
        "                 print(f\"No products found on page {page_num}. Stopping.\")\n",
        "                 break # Stop if no products are found\n",
        "\n",
        "            products_on_page = await parse_amazon_page(page) # Use await\n",
        "            all_products.extend(products_on_page)\n",
        "            await asyncio.sleep(1 + (page_num % 2)) # small variable delay\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    # --- Save results ---\n",
        "    df = pd.DataFrame(all_products)\n",
        "    df.to_csv(\"amazon_ram_playwright.csv\", index=False)\n",
        "    print(\"Scraping complete! Saved to amazon_ram_playwright.csv\")\n",
        "\n",
        "# Run the scraper\n",
        "await scrape_amazon_ram(max_pages=75) # Adjust max_pages as needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMRMrOk2n-Dw",
        "outputId": "d9207301-7bf1-47b0-8eb6-b55617a48a26",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping page 51...\n",
            "Scraping page 52...\n",
            "Scraping page 53...\n",
            "Scraping page 54...\n",
            "Scraping page 55...\n",
            "Scraping page 56...\n",
            "Scraping page 57...\n",
            "Scraping page 58...\n",
            "Scraping page 59...\n",
            "Scraping page 60...\n",
            "Scraping page 61...\n",
            "Scraping page 62...\n",
            "Scraping page 63...\n",
            "Scraping page 64...\n",
            "Scraping page 65...\n",
            "Scraping page 66...\n",
            "Scraping page 67...\n",
            "Scraping page 68...\n",
            "Scraping page 69...\n",
            "Scraping page 70...\n",
            "Scraping page 71...\n",
            "Scraping page 72...\n",
            "Scraping page 73...\n",
            "Scraping page 74...\n",
            "Scraping page 75...\n",
            "Scraping complete! Saved to amazon_ram_playwright.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LDLC Scraping**"
      ],
      "metadata": {
        "id": "kZQSbqjGuuQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CPUs**"
      ],
      "metadata": {
        "id": "bUpze7wqvPBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "# ---------------------- SPEC EXTRACTOR ----------------------\n",
        "def extract_specs(text):\n",
        "    text = text.replace(\",\", \".\").replace(\"-\", \" \")\n",
        "    specs = {}\n",
        "\n",
        "    # Cores\n",
        "    match = re.search(r'(\\d+)\\s*(?:core|coeur|coeurs|cores?)\\b', text, re.I)\n",
        "    specs[\"Cores\"] = match.group(1) if match else None\n",
        "\n",
        "    # Threads\n",
        "    match = re.search(r'(\\d+)\\s*(?:thread|threads|T)\\b', text, re.I)\n",
        "    specs[\"Threads\"] = match.group(1) if match else None\n",
        "\n",
        "    # Frequency (GHz)\n",
        "    match = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(?:GHz|GHZ)', text, re.I)\n",
        "    specs[\"Frequency (GHz)\"] = match.group(1) if match else None\n",
        "\n",
        "    # Cache (MB)\n",
        "    match = re.search(r'(\\d+)\\s*(?:MB|Mo)\\s*(?:cache|L\\d)?', text, re.I)\n",
        "    specs[\"Cache (MB)\"] = match.group(1) if match else None\n",
        "\n",
        "    # Socket\n",
        "    match = re.search(r'(AM\\d+|LGA\\s*\\d+)', text, re.I)\n",
        "    specs[\"Socket\"] = match.group(1).replace(\" \", \"\") if match else None\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "# ---------------------- SCRAPER FUNCTION ----------------------\n",
        "async def scrape_ldlc_cpus():\n",
        "    start_url = \"https://www.ldlc.com/informatique/pieces-informatique/processeur/c4300\"\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        current_url = start_url\n",
        "        while True:\n",
        "            print(f\"\\n Loading page {page_number}: {current_url}\")\n",
        "            await page.goto(current_url)\n",
        "            await page.wait_for_load_state(\"networkidle\")\n",
        "\n",
        "            html = await page.content()\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            products = soup.select(\"li.pdt-item\")\n",
        "\n",
        "            if not products:\n",
        "                print(\"No more products found.\")\n",
        "                break\n",
        "\n",
        "            for item in products:\n",
        "                name_tag = item.select_one(\"h3.title-3 a\")\n",
        "                desc_tag = item.select_one(\"p.desc\")\n",
        "                price_tag = item.select_one(\"div.price\")\n",
        "\n",
        "                name = name_tag.get_text(strip=True) if name_tag else None\n",
        "                desc = desc_tag.get_text(strip=True) if desc_tag else \"\"\n",
        "                price = price_tag.get_text(strip=True) if price_tag else \"N/A\"\n",
        "                link = (\n",
        "                    \"https://www.ldlc.com\" + name_tag[\"href\"]\n",
        "                    if name_tag and name_tag.has_attr(\"href\")\n",
        "                    else None\n",
        "                )\n",
        "\n",
        "                # Extract structured specs\n",
        "                specs = extract_specs(f\"{name} {desc}\")\n",
        "\n",
        "                all_data.append({\n",
        "                    \"Name\": name,\n",
        "                    \"Description\": desc,\n",
        "                    \"Price\": price,\n",
        "                    \"URL\": link,\n",
        "                    **specs\n",
        "                })\n",
        "\n",
        "            # Try to find pagination button\n",
        "            next_button = soup.select_one(\"a.page-suivante\")\n",
        "            if next_button and \"href\" in next_button.attrs:\n",
        "                next_url = \"https://www.ldlc.com\" + next_button[\"href\"]\n",
        "                current_url = next_url\n",
        "                page_number += 1\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(\" All pages scraped.\")\n",
        "                break\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    # Save results\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.drop_duplicates(subset=[\"Name\"], inplace=True)\n",
        "    df.to_csv(\"ldlc_cpus.csv\", index=False)\n",
        "    print(f\"\\n Scraping complete! {len(df)} CPUs saved to ldlc_cpus.csv.\")\n",
        "\n",
        "\n",
        "# ---------------------- RUN ----------------------\n",
        "await scrape_ldlc_cpus()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXqtGrX5tZOa",
        "outputId": "26bd7cb6-0ed3-4c76-c107-d4e40679c916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔄 Loading page 1: https://www.ldlc.com/informatique/pieces-informatique/processeur/c4300\n",
            "✅ All pages scraped.\n",
            "\n",
            "💾 Scraping complete! 44 CPUs saved to ldlc_cpus.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPUs**"
      ],
      "metadata": {
        "id": "pXEUruqEvI07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "\n",
        "# ---------------------- SPEC EXTRACTOR ----------------------\n",
        "def extract_gpu_specs(text):\n",
        "    text = text.replace(\",\", \".\").replace(\"-\", \" \").lower()\n",
        "    specs = {}\n",
        "\n",
        "    # Memory (Go / GB)\n",
        "    match = re.search(r'(\\d+)\\s*(?:go|gb)\\s*(?:gddr\\d*)?', text, re.I)\n",
        "    specs[\"Memory (GB)\"] = match.group(1) if match else None\n",
        "\n",
        "    # Memory Type\n",
        "    match = re.search(r'(gddr\\d+)', text, re.I)\n",
        "    specs[\"Memory Type\"] = match.group(1).upper() if match else None\n",
        "\n",
        "    # Chipset (RTX, GTX, Radeon, etc.)\n",
        "    match = re.search(r'(rtx\\s*\\d{3,4}|gtx\\s*\\d{3,4}|rx\\s*\\d{3,4}|arc\\s*\\w+)', text, re.I)\n",
        "    specs[\"Chipset\"] = match.group(1).upper().replace(\" \", \"\") if match else None\n",
        "\n",
        "    # Bus / PCI Express\n",
        "    match = re.search(r'(pci\\s*express\\s*\\d*\\.?\\d*)', text, re.I)\n",
        "    specs[\"Bus Interface\"] = match.group(1).upper().replace(\" \", \"\") if match else None\n",
        "\n",
        "    # Ports\n",
        "    ports = []\n",
        "    if re.search(r'hdmi', text, re.I): ports.append(\"HDMI\")\n",
        "    if re.search(r'displayport', text, re.I): ports.append(\"DisplayPort\")\n",
        "    if re.search(r'dvi', text, re.I): ports.append(\"DVI\")\n",
        "    if ports:\n",
        "        specs[\"Ports\"] = \", \".join(ports)\n",
        "\n",
        "    # Cooling type\n",
        "    match = re.search(r'(ventilateur|blower|fan|watercooling|radiateur)', text, re.I)\n",
        "    specs[\"Cooling\"] = match.group(1).capitalize() if match else None\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "# ---------------------- SCRAPER FUNCTION ----------------------\n",
        "async def scrape_ldlc_gpus():\n",
        "    start_url = \"https://www.ldlc.com/informatique/pieces-informatique/carte-graphique-interne/c4684\"\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        current_url = start_url\n",
        "        while True:\n",
        "            print(f\"\\n Loading page {page_number}: {current_url}\")\n",
        "            await page.goto(current_url)\n",
        "            await page.wait_for_load_state(\"networkidle\")\n",
        "\n",
        "            html = await page.content()\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            products = soup.select(\"li.pdt-item\")\n",
        "\n",
        "            if not products:\n",
        "                print(\" No more products found.\")\n",
        "                break\n",
        "\n",
        "            for item in products:\n",
        "                name_tag = item.select_one(\"h3.title-3 a\")\n",
        "                desc_tag = item.select_one(\"p.desc\")\n",
        "                price_tag = item.select_one(\"div.price\")\n",
        "\n",
        "                name = name_tag.get_text(strip=True) if name_tag else None\n",
        "                desc = desc_tag.get_text(strip=True) if desc_tag else \"\"\n",
        "                price = price_tag.get_text(strip=True) if price_tag else \"N/A\"\n",
        "                link = (\n",
        "                    \"https://www.ldlc.com\" + name_tag[\"href\"]\n",
        "                    if name_tag and name_tag.has_attr(\"href\")\n",
        "                    else None\n",
        "                )\n",
        "\n",
        "                # Structured GPU specs\n",
        "                specs = extract_gpu_specs(f\"{name} {desc}\")\n",
        "\n",
        "                all_data.append({\n",
        "                    \"Name\": name,\n",
        "                    \"Description\": desc,\n",
        "                    \"Price\": price,\n",
        "                    \"URL\": link,\n",
        "                    **specs\n",
        "                })\n",
        "\n",
        "            # Try to find pagination button\n",
        "            next_button = soup.select_one(\"a.page-suivante\")\n",
        "            if next_button and \"href\" in next_button.attrs:\n",
        "                next_url = \"https://www.ldlc.com\" + next_button[\"href\"]\n",
        "                current_url = next_url\n",
        "                page_number += 1\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(\"\\n All pages scraped.\")\n",
        "                break\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    # Save results\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.drop_duplicates(subset=[\"Name\"], inplace=True)\n",
        "    df.to_csv(\"ldlc_gpus.csv\", index=False)\n",
        "    print(f\"\\n Scraping complete! {len(df)} GPUs saved to ldlc_gpus.csv.\")\n",
        "\n",
        "\n",
        "# ---------------------- RUN ----------------------\n",
        "await scrape_ldlc_gpus()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3siPaKEEu1ky",
        "outputId": "dffbef9c-bfce-4798-e6ac-9c8c974bd9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧭 Loading page 1: https://www.ldlc.com/informatique/pieces-informatique/carte-graphique-interne/c4684\n",
            "\n",
            "✅ All pages scraped.\n",
            "\n",
            "💾 Scraping complete! 48 GPUs saved to ldlc_gpus.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MotherBoards**"
      ],
      "metadata": {
        "id": "2OKzExYDz_Jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "# ---------------------- SPEC EXTRACTOR ----------------------\n",
        "def extract_motherboard_specs(text):\n",
        "    text = text.replace(\",\", \".\").replace(\"-\", \" \")\n",
        "    specs = {}\n",
        "\n",
        "    # Socket\n",
        "    match = re.search(r'(LGA\\s*\\d+|AM\\d+|sTRX\\d+|s\\d+)', text, re.I)\n",
        "    specs[\"Socket\"] = match.group(1).replace(\" \", \"\") if match else None\n",
        "\n",
        "    # Chipset\n",
        "    match = re.search(r'\\b(B\\d{3}|Z\\d{3}|H\\d{3}|A\\d{3}|X\\d{3}|A520|B550|X570|H610|B760|Z790)\\b', text, re.I)\n",
        "    specs[\"Chipset\"] = match.group(1).upper() if match else None\n",
        "\n",
        "    # RAM Type\n",
        "    match = re.search(r'(DDR[3-5])', text, re.I)\n",
        "    specs[\"RAM Type\"] = match.group(1).upper() if match else None\n",
        "\n",
        "    # Form Factor\n",
        "    match = re.search(r'(ATX|Micro\\s*ATX|Mini\\s*ITX)', text, re.I)\n",
        "    specs[\"Form Factor\"] = match.group(1).replace(\" \", \"\") if match else None\n",
        "\n",
        "    # M.2 Slots\n",
        "    match = re.search(r'(\\d+)\\s*x?\\s*M\\.2', text, re.I)\n",
        "    specs[\"M.2 Slots\"] = match.group(1) if match else None\n",
        "\n",
        "    # PCIe version\n",
        "    match = re.search(r'(PCI[-\\s]*Express\\s*(\\d+(\\.\\d+)?))', text, re.I)\n",
        "    specs[\"PCIe\"] = match.group(1).replace(\" \", \"\") if match else None\n",
        "\n",
        "    # USB version\n",
        "    match = re.search(r'(USB\\s*3\\.2|USB\\s*3\\.1|USB\\s*3\\.0|USB\\s*2\\.0)', text, re.I)\n",
        "    specs[\"USB\"] = match.group(1).replace(\" \", \"\") if match else None\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "# ---------------------- SCRAPER FUNCTION ----------------------\n",
        "async def scrape_ldlc_motherboards():\n",
        "    start_url = \"https://www.ldlc.com/informatique/pieces-informatique/carte-mere/c4293\"\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        current_url = start_url\n",
        "        while True:\n",
        "            print(f\"\\n Loading page {page_number}: {current_url}\")\n",
        "            try:\n",
        "                await page.goto(current_url)\n",
        "                await page.wait_for_load_state(\"networkidle\")\n",
        "            except Exception as e:\n",
        "                print(f\"Timeout or error: {e}\")\n",
        "                break\n",
        "\n",
        "            html = await page.content()\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            products = soup.select(\"li.pdt-item\")\n",
        "\n",
        "            if not products:\n",
        "                print(\"No more products found.\")\n",
        "                break\n",
        "\n",
        "            for item in products:\n",
        "                name_tag = item.select_one(\"h3.title-3 a\")\n",
        "                desc_tag = item.select_one(\"p.desc\")\n",
        "                price_tag = item.select_one(\"div.price\")\n",
        "\n",
        "                name = name_tag.get_text(strip=True) if name_tag else None\n",
        "                desc = desc_tag.get_text(strip=True) if desc_tag else \"\"\n",
        "                price = price_tag.get_text(strip=True).replace(\"\\u202f\", \"\") if price_tag else \"N/A\"\n",
        "                link = (\n",
        "                    \"https://www.ldlc.com\" + name_tag[\"href\"]\n",
        "                    if name_tag and name_tag.has_attr(\"href\")\n",
        "                    else None\n",
        "                )\n",
        "\n",
        "                specs = extract_motherboard_specs(f\"{name} {desc}\")\n",
        "\n",
        "                all_data.append({\n",
        "                    \"Name\": name,\n",
        "                    \"Description\": desc,\n",
        "                    \"Price\": price,\n",
        "                    \"URL\": link,\n",
        "                    **specs\n",
        "                })\n",
        "\n",
        "            # Check pagination\n",
        "            next_button = soup.select_one(\"a.page-suivante\")\n",
        "            if next_button and \"href\" in next_button.attrs:\n",
        "                next_url = \"https://www.ldlc.com\" + next_button[\"href\"]\n",
        "                current_url = next_url\n",
        "                page_number += 1\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(\"All pages scraped.\")\n",
        "                break\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    # Save results\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.drop_duplicates(subset=[\"Name\"], inplace=True)\n",
        "    df.to_csv(\"ldlc_motherboards.csv\", index=False)\n",
        "    print(f\"\\n Scraping complete! {len(df)} motherboards saved to ldlc_motherboards.csv.\")\n",
        "\n",
        "\n",
        "# ---------------------- RUN ----------------------\n",
        "await scrape_ldlc_motherboards()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBaQRzn7wPPz",
        "outputId": "2e6962e0-70b2-47da-f6ba-6d7bdd6bf6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧭 Loading page 1: https://www.ldlc.com/informatique/pieces-informatique/carte-mere/c4293\n",
            "✅ All pages scraped.\n",
            "\n",
            "💾 Scraping complete! 48 motherboards saved to ldlc_motherboards.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ram**"
      ],
      "metadata": {
        "id": "pkaV-vVr0oVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "# ---------------------- SPEC EXTRACTOR ----------------------\n",
        "def extract_ram_specs(text):\n",
        "    text = text.replace(\",\", \".\").replace(\"-\", \" \").lower()\n",
        "    specs = {}\n",
        "\n",
        "    # Capacity (GB)\n",
        "    match = re.search(r'(\\d+)\\s*(go|gb)', text, re.I)\n",
        "    specs[\"Capacity (GB)\"] = match.group(1) if match else None\n",
        "\n",
        "    # Number of sticks / modules\n",
        "    match = re.search(r'\\(?(\\d+)\\s*x\\s*\\d+\\s*go\\)?', text, re.I)\n",
        "    specs[\"Modules\"] = match.group(1) if match else \"1\"\n",
        "\n",
        "    # RAM type (DDR3/DDR4/DDR5)\n",
        "    match = re.search(r'(ddr[3-5])', text, re.I)\n",
        "    specs[\"Type\"] = match.group(1).upper() if match else None\n",
        "\n",
        "    # Frequency (MHz)\n",
        "    match = re.search(r'(\\d{3,5})\\s*mhz', text, re.I)\n",
        "    specs[\"Frequency (MHz)\"] = match.group(1) if match else None\n",
        "\n",
        "    # CAS Latency (CL)\n",
        "    match = re.search(r'cl(\\d+)', text, re.I)\n",
        "    specs[\"CAS Latency\"] = match.group(1) if match else None\n",
        "\n",
        "    # RGB\n",
        "    specs[\"RGB\"] = \"Yes\" if re.search(r'rgb', text, re.I) else \"No\"\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "# ---------------------- SCRAPER FUNCTION ----------------------\n",
        "async def scrape_ldlc_ram():\n",
        "    start_url = \"https://www.ldlc.com/informatique/pieces-informatique/memoire-pc/c4703\"\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        current_url = start_url\n",
        "        while True:\n",
        "            print(f\"\\n Loading page {page_number}: {current_url}\")\n",
        "            try:\n",
        "                await page.goto(current_url)\n",
        "                await page.wait_for_load_state(\"networkidle\")\n",
        "            except Exception as e:\n",
        "                print(f\" Timeout or error: {e}\")\n",
        "                break\n",
        "\n",
        "            html = await page.content()\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            products = soup.select(\"li.pdt-item\")\n",
        "\n",
        "            if not products:\n",
        "                print(\" No products found on this page.\")\n",
        "                break\n",
        "\n",
        "            print(f\" Found {len(products)} products on page {page_number}\")\n",
        "\n",
        "            for item in products:\n",
        "                name_tag = item.select_one(\"h3.title-3 a\")\n",
        "                desc_tag = item.select_one(\"p.desc\")\n",
        "                price_tag = item.select_one(\"div.price\")\n",
        "\n",
        "                name = name_tag.get_text(strip=True) if name_tag else None\n",
        "                desc = desc_tag.get_text(strip=True) if desc_tag else \"\"\n",
        "                price = price_tag.get_text(strip=True).replace(\"\\u202f\", \"\") if price_tag else \"N/A\"\n",
        "                link = (\n",
        "                    \"https://www.ldlc.com\" + name_tag[\"href\"]\n",
        "                    if name_tag and name_tag.has_attr(\"href\")\n",
        "                    else None\n",
        "                )\n",
        "\n",
        "                specs = extract_ram_specs(f\"{name} {desc}\")\n",
        "\n",
        "                all_data.append({\n",
        "                    \"Name\": name,\n",
        "                    \"Description\": desc,\n",
        "                    \"Price\": price,\n",
        "                    \"URL\": link,\n",
        "                    **specs\n",
        "                })\n",
        "\n",
        "            # Check for next page\n",
        "            next_button = soup.select_one(\"a.page-suivante\")\n",
        "            if next_button and \"href\" in next_button.attrs:\n",
        "                next_url = \"https://www.ldlc.com\" + next_button[\"href\"]\n",
        "                current_url = next_url\n",
        "                page_number += 1\n",
        "                time.sleep(2)\n",
        "            else:\n",
        "                print(\" All pages scraped.\")\n",
        "                break\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    # Save all data\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df.drop_duplicates(subset=[\"Name\"], inplace=True)\n",
        "    df.to_csv(\"ldlc_ram.csv\", index=False)\n",
        "    print(f\"\\n Scraping complete! {len(df)} RAM modules saved to ldlc_ram.csv.\")\n",
        "\n",
        "\n",
        "# ---------------------- RUN ----------------------\n",
        "await scrape_ldlc_ram()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cnq6ZW3x0C1D",
        "outputId": "28c8d2ab-d8ce-43c1-f899-89f075739f23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🧭 Loading page 1: https://www.ldlc.com/informatique/pieces-informatique/memoire-pc/c4703\n",
            "✅ Found 48 products on page 1\n",
            "✅ All pages scraped.\n",
            "\n",
            "💾 Scraping complete! 45 RAM modules saved to ldlc_ram.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neweeg"
      ],
      "metadata": {
        "id": "NKhob2UH2hhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CPUs**"
      ],
      "metadata": {
        "id": "A2BK9xjd2oM5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f30789ee",
        "outputId": "ca7f8755-fdcd-4498-a5ee-35a83bb30f53"
      },
      "source": [
        "!pip install playwright\n",
        "!playwright install"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: playwright in /usr/local/lib/python3.12/dist-packages (1.55.0)\n",
            "Requirement already satisfied: pyee<14,>=13 in /usr/local/lib/python3.12/dist-packages (from playwright) (13.0.0)\n",
            "Requirement already satisfied: greenlet<4.0.0,>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from playwright) (3.2.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from pyee<14,>=13->playwright) (4.15.0)\n",
            "Playwright Host validation warning: \n",
            "╔══════════════════════════════════════════════════════╗\n",
            "║ Host system is missing dependencies to run browsers. ║\n",
            "║ Missing libraries:                                   ║\n",
            "║     libwoff2dec.so.1.0.2                             ║\n",
            "║     libgstgl-1.0.so.0                                ║\n",
            "║     libgstcodecparsers-1.0.so.0                      ║\n",
            "║     libavif.so.13                                    ║\n",
            "║     libharfbuzz-icu.so.0                             ║\n",
            "║     libenchant-2.so.2                                ║\n",
            "║     libsecret-1.so.0                                 ║\n",
            "║     libhyphen.so.0                                   ║\n",
            "║     libmanette-0.2.so.0                              ║\n",
            "╚══════════════════════════════════════════════════════╝\n",
            "    at validateDependenciesLinux (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/dependencies.js:269:9)\n",
            "\u001b[90m    at process.processTicksAndRejections (node:internal/process/task_queues:105:5)\u001b[39m\n",
            "    at async Registry._validateHostRequirements (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:934:14)\n",
            "    at async Registry._validateHostRequirementsForExecutableIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1056:7)\n",
            "    at async Registry.validateHostRequirementsForExecutablesIfNeeded (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/server/registry/index.js:1045:7)\n",
            "    at async i.<anonymous> (/usr/local/lib/python3.12/dist-packages/playwright/driver/package/lib/cli/program.js:217:7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **motherboards**"
      ],
      "metadata": {
        "id": "pGqu-EQezFE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm.asyncio import tqdm\n",
        "import time\n",
        "\n",
        "# MOTHERBOARD SPEC EXTRACTOR\n",
        "def extract_motherboard_specs(text):\n",
        "    text = text.replace(\",\", \".\").replace(\"-\", \" \").lower()\n",
        "    specs = {}\n",
        "\n",
        "    # Socket\n",
        "    socket_match = re.search(r'(am\\d+|lga\\s*\\d+|socket\\s*\\w+)', text, re.I)\n",
        "    specs[\"Socket\"] = socket_match.group(1).replace(\" \", \"\").upper() if socket_match else None\n",
        "\n",
        "    # Chipset\n",
        "    chipset_match = re.search(r'(b\\d+|x\\d+|z\\d+|h\\d+|a\\d+)(?:\\s*\\w*)?', text, re.I)\n",
        "    specs[\"Chipset\"] = chipset_match.group(1).upper() if chipset_match else None\n",
        "\n",
        "    # Form Factor\n",
        "    form_factors = [\"atx\", \"micro atx\", \"mini itx\", \"extended atx\", \"m-atx\", \"itx\"]\n",
        "    for ff in form_factors:\n",
        "        if ff in text:\n",
        "            specs[\"Form Factor\"] = ff.upper()\n",
        "            break\n",
        "\n",
        "    # Memory Type\n",
        "    memory_match = re.search(r'(ddr\\d+)', text, re.I)\n",
        "    specs[\"Memory Type\"] = memory_match.group(1).upper() if memory_match else None\n",
        "\n",
        "    # M.2 Slots\n",
        "    m2_match = re.search(r'(\\d+)\\s*x?\\s*m\\.?2', text, re.I)\n",
        "    specs[\"M.2 Slots\"] = m2_match.group(1) if m2_match else None\n",
        "\n",
        "    # PCIe Version\n",
        "    pcie_match = re.search(r'pcie\\s*(\\d+\\.?\\d?)', text, re.I)\n",
        "    specs[\"PCIe Version\"] = pcie_match.group(1) if pcie_match else None\n",
        "\n",
        "    # WiFi\n",
        "    wifi_match = re.search(r'(wifi\\s*\\d?\\w?|wireless\\s*\\w+)', text, re.I)\n",
        "    specs[\"WiFi\"] = wifi_match.group(1).upper() if wifi_match else None\n",
        "\n",
        "    # LAN Speed\n",
        "    lan_match = re.search(r'(\\d+\\.?\\d?)\\s*gb(e|)\\s*lan', text, re.I)\n",
        "    specs[\"LAN Speed\"] = lan_match.group(1) + \"Gb\" if lan_match else None\n",
        "\n",
        "    return specs\n",
        "\n",
        "async def scrape_newegg_motherboards():\n",
        "    start_url = \"https://www.newegg.com/p/pl?N=100006654\"\n",
        "    all_data = []\n",
        "    page_number = 1\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        # Set user agent to avoid blocking\n",
        "        await page.set_extra_http_headers({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "        })\n",
        "\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        current_url = start_url\n",
        "        while page_number <= 3:  # Limit to 3 pages for demo, remove for full scrape\n",
        "            print(f\"\\nLoading page {page_number}: {current_url}\")\n",
        "\n",
        "            try:\n",
        "                await page.goto(current_url, wait_until='networkidle')\n",
        "                await page.wait_for_selector(\".item-cell\", timeout=30000)\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading page: {e}\")\n",
        "                break\n",
        "\n",
        "            html = await page.content()\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "\n",
        "            # Find all motherboard items\n",
        "            products = soup.select(\".item-cell\")\n",
        "            print(f\"Found {len(products)} products on page {page_number}\")\n",
        "\n",
        "            for product in tqdm(products, desc=f\"Extracting page {page_number}\"):\n",
        "                try:\n",
        "                    # Extract product name\n",
        "                    title_tag = product.select_one(\".item-title\")\n",
        "                    if not title_tag:\n",
        "                        continue\n",
        "\n",
        "                    name = title_tag.get_text(strip=True)\n",
        "\n",
        "                    # Extract product URL\n",
        "                    link_tag = product.select_one(\".item-title\")\n",
        "                    if link_tag and link_tag.get('href'):\n",
        "                        link = link_tag['href']\n",
        "                        if not link.startswith('http'):\n",
        "                            link = \"https://www.newegg.com\" + link\n",
        "                    else:\n",
        "                        link = \"N/A\"\n",
        "\n",
        "                    # Extract price\n",
        "                    price_tag = product.select_one(\".price-current strong\")\n",
        "                    price_decimal = product.select_one(\".price-current sup\")\n",
        "\n",
        "                    price = \"N/A\"\n",
        "                    if price_tag:\n",
        "                        price = f\"${price_tag.get_text(strip=True)}\"\n",
        "                        if price_decimal:\n",
        "                            price += f\".{price_decimal.get_text(strip=True)}\"\n",
        "\n",
        "                    # Extract brand\n",
        "                    brand_tag = product.select_one(\".item-brand img\")\n",
        "                    brand = brand_tag.get('title') if brand_tag else \"N/A\"\n",
        "\n",
        "                    # Extract rating\n",
        "                    rating_tag = product.select_one(\".item-rating\")\n",
        "                    rating = \"N/A\"\n",
        "                    rating_count = \"N/A\"\n",
        "                    if rating_tag:\n",
        "                        rating_span = rating_tag.select_one(\".item-rating-num\")\n",
        "                        if rating_span:\n",
        "                            rating_count = rating_span.get_text(strip=True).strip('()')\n",
        "\n",
        "                    # Extract shipping info\n",
        "                    shipping_tag = product.select_one(\".product-price-ship-eligible\")\n",
        "                    shipping = shipping_tag.get_text(strip=True) if shipping_tag else \"N/A\"\n",
        "\n",
        "                    # Combine text for spec extraction\n",
        "                    description_text = f\"{name} {brand}\"\n",
        "\n",
        "                    # Extract features list\n",
        "                    features_tag = product.select_one(\".item-features\")\n",
        "                    features_text = \"\"\n",
        "                    if features_tag:\n",
        "                        features_items = features_tag.find_all(\"li\")\n",
        "                        for item in features_items:\n",
        "                            features_text += \" \" + item.get_text(strip=True)\n",
        "\n",
        "                    full_text = f\"{description_text} {features_text}\"\n",
        "\n",
        "                    specs = extract_motherboard_specs(full_text)\n",
        "\n",
        "                    all_data.append({\n",
        "                        \"Name\": name,\n",
        "                        \"Brand\": brand,\n",
        "                        \"Price\": price,\n",
        "                        \"URL\": link,\n",
        "                        \"Rating Count\": rating_count,\n",
        "                        \"Shipping\": shipping,\n",
        "                        **specs\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error extracting product: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Pagination - find next page\n",
        "            next_button = soup.select_one(\"a.btn[title='Next']\")\n",
        "            if next_button and \"href\" in next_button.attrs and \"is-disabled\" not in next_button.get(\"class\", []):\n",
        "                next_href = next_button[\"href\"]\n",
        "                current_url = next_href\n",
        "                page_number += 1\n",
        "                await asyncio.sleep(2)  # Be respectful with delays\n",
        "            else:\n",
        "                print(\"No more pages found.\")\n",
        "                break\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    # Clean and export\n",
        "    if all_data:\n",
        "        df = pd.DataFrame(all_data)\n",
        "        df.drop_duplicates(subset=[\"Name\"], inplace=True)\n",
        "        df.to_csv(\"newegg_motherboards.csv\", index=False)\n",
        "        print(f\"\\nScraping complete! {len(df)} motherboards saved to newegg_motherboards.csv\")\n",
        "\n",
        "        # Display sample of data\n",
        "        print(\"\\nSample of scraped data:\")\n",
        "        print(df.head()[[\"Name\", \"Brand\", \"Price\", \"Socket\", \"Chipset\"]])\n",
        "    else:\n",
        "        print(\"No data was scraped.\")\n",
        "\n",
        "# Run the scraper\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(scrape_newegg_motherboards())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh2_qIbv0mTY",
        "outputId": "a8b80b6e-5e5e-4c06-8e1a-373efba65dde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading page 1: https://www.newegg.com/p/pl?N=100006654\n",
            "Found 36 products on page 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting page 1: 100%|██████████| 36/36 [00:00<00:00, 269.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading page 2: https://www.newegg.com/p/pl?N=100006654&page=2\n",
            "Error loading page: Page.goto: Timeout 60000ms exceeded.\n",
            "Call log:\n",
            "  - navigating to \"https://www.newegg.com/p/pl?N=100006654&page=2\", waiting until \"networkidle\"\n",
            "\n",
            "\n",
            "Scraping complete! 36 motherboards saved to newegg_motherboards.csv\n",
            "\n",
            "Sample of scraped data:\n",
            "                                                Name     Brand     Price  \\\n",
            "0  MSI MAG X870 TOMAHAWK WIFI AM5 AMD X870 ATX Mo...       MSI  $309..99   \n",
            "1  GIGABYTE B650M GAMING PLUS WIFI AM5 LGA 1718 A...  GIGABYTE  $129..99   \n",
            "2  GIGABYTE B550M GAMING X WIFI6 AM4 AMD B550 Mic...  GIGABYTE  $104..99   \n",
            "3  ASUS B850 MAX GAMING WIFI W - AMD AM5 ATX Moth...      ASUS  $169..99   \n",
            "4  ASRock B650M Pro RS WiFi AM5 AMD B650 Micro AT...    ASRock  $129..99   \n",
            "\n",
            "  Socket Chipset  \n",
            "0    AM5    X870  \n",
            "1    AM5    B650  \n",
            "2    AM4    B550  \n",
            "3    AM5    B850  \n",
            "4    AM5    B650  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ram**"
      ],
      "metadata": {
        "id": "8RnwiJuiIKVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError # Import TimeoutError\n",
        "import pandas as pd\n",
        "import time # Import time\n",
        "\n",
        "async def scrape_newegg_ram():\n",
        "    base_url = \"https://www.newegg.com/p/pl?N=100006650&page=\"\n",
        "    all_products = []\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "        await page.set_extra_http_headers({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
        "                          \"(KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n",
        "        })\n",
        "\n",
        "        # Increase default timeout to 60 seconds\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        for page_num in range(1, 6):  # scrape first 5 pages\n",
        "            print(f\"🔍 Scraping page {page_num}...\")\n",
        "            url = f\"{base_url}{page_num}\"\n",
        "            try:\n",
        "                await page.goto(url, wait_until=\"networkidle\", timeout=60000) # Set timeout for goto\n",
        "            except PlaywrightTimeoutError: # Catch the specific timeout error\n",
        "                print(f\"⚠️ Timeout loading page {page_num}. Skipping to next page.\")\n",
        "                continue # Skip to the next page if timeout occurs\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Error loading page {page_num}: {e}. Stopping.\")\n",
        "                break # Stop for other errors\n",
        "\n",
        "\n",
        "            product_elements = await page.query_selector_all(\"div.item-cell\")\n",
        "            if not product_elements:\n",
        "                print(f\"⚠️ No products found on page {page_num}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            for product in product_elements:\n",
        "                try:\n",
        "                    # Product name & link\n",
        "                    title_el = await product.query_selector(\"a.item-title\")\n",
        "                    name = await title_el.inner_text() if title_el else None\n",
        "                    link = await title_el.get_attribute(\"href\") if title_el else None\n",
        "\n",
        "                    # Specs (inside ul.item-features)\n",
        "                    specs = {}\n",
        "                    feature_items = await product.query_selector_all(\"ul.item-features li\")\n",
        "                    for li in feature_items:\n",
        "                        text = await li.inner_text()\n",
        "                        if \":\" in text:\n",
        "                            key, value = text.split(\":\", 1)\n",
        "                            specs[key.strip()] = value.strip()\n",
        "\n",
        "                    # Price\n",
        "                    price_el = await product.query_selector(\"li.price-current\")\n",
        "                    price = None\n",
        "                    if price_el:\n",
        "                        price_text = await price_el.inner_text()\n",
        "                        price = price_text.replace(\"\\n\", \"\").replace(\"–\", \"\").strip()\n",
        "\n",
        "                    all_products.append({\n",
        "                        \"Name\": name,\n",
        "                        \"CAS Latency\": specs.get(\"CAS Latency\"),\n",
        "                        \"Voltage\": specs.get(\"Voltage\"),\n",
        "                        \"Multi-channel Kit\": specs.get(\"Multi-channel Kit\"),\n",
        "                        \"Timing\": specs.get(\"Timing\"),\n",
        "                        \"Model\": specs.get(\"Model #\"),\n",
        "                        \"Price\": price,\n",
        "                        \"Link\": link\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error extracting product: {e}\")\n",
        "                    continue\n",
        "            await asyncio.sleep(2) # Add a small delay between pages\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    df = pd.DataFrame(all_products)\n",
        "    df.to_csv(\"newegg_ram.csv\", index=False)\n",
        "    print(f\"✅ Done! {len(df)} RAM products saved to newegg_ram.csv\")\n",
        "\n",
        "# Run it\n",
        "await scrape_newegg_ram()"
      ],
      "metadata": {
        "id": "awnE1nRn5kYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cce4af6-6096-4b55-fd4f-51ae84ab8ec4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Scraping page 1...\n",
            "🔍 Scraping page 2...\n",
            "⚠️ Timeout loading page 2. Skipping to next page.\n",
            "🔍 Scraping page 3...\n",
            "🔍 Scraping page 4...\n",
            "⚠️ Timeout loading page 4. Skipping to next page.\n",
            "🔍 Scraping page 5...\n",
            "⚠️ Timeout loading page 5. Skipping to next page.\n",
            "✅ Done! 72 RAM products saved to newegg_ram.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPUs**"
      ],
      "metadata": {
        "id": "9ZCEOFCKJy-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "\n",
        "# === Function to extract GPU specs from title text ===\n",
        "def extract_gpu_specs(name):\n",
        "    specs = {}\n",
        "    if not name:\n",
        "        return specs\n",
        "\n",
        "    name = name.replace(\"-\", \" \").replace(\",\", \" \")\n",
        "\n",
        "    # GPU model (e.g., RTX 4070 Ti, RX 7800 XT)\n",
        "    gpu_match = re.search(r\"(RTX|RX|GTX)\\s*\\d{3,4}\\s*(Ti|SUPER|XT|XTX)?\", name, re.IGNORECASE)\n",
        "    specs[\"GPU Model\"] = gpu_match.group(0).upper() if gpu_match else None\n",
        "\n",
        "    # Memory (e.g., 16GB, 12 GB GDDR6)\n",
        "    mem_match = re.search(r\"(\\d{1,3})\\s*GB\", name, re.IGNORECASE)\n",
        "    specs[\"Memory Size (GB)\"] = mem_match.group(1) if mem_match else None\n",
        "\n",
        "    # Memory type (GDDR6, GDDR6X, GDDR7)\n",
        "    mem_type = re.search(r\"(GDDR6X|GDDR6|GDDR7|GDDR5X|GDDR5)\", name, re.IGNORECASE)\n",
        "    specs[\"Memory Type\"] = mem_type.group(1).upper() if mem_type else None\n",
        "\n",
        "    # PCIe version\n",
        "    pcie_match = re.search(r\"PCI(?:\\s*Express|\\s*E)?\\s*(\\d+\\.?\\d*)\", name, re.IGNORECASE)\n",
        "    specs[\"PCIe Version\"] = pcie_match.group(1) if pcie_match else None\n",
        "\n",
        "    # Brand extraction (NVIDIA, AMD, Intel if appears)\n",
        "    if re.search(r\"RTX|GTX\", name, re.IGNORECASE):\n",
        "        specs[\"Chipset Brand\"] = \"NVIDIA\"\n",
        "    elif re.search(r\"RX|RADEON\", name, re.IGNORECASE):\n",
        "        specs[\"Chipset Brand\"] = \"AMD\"\n",
        "    elif re.search(r\"ARC\", name, re.IGNORECASE):\n",
        "        specs[\"Chipset Brand\"] = \"Intel\"\n",
        "    else:\n",
        "        specs[\"Chipset Brand\"] = None\n",
        "\n",
        "    return specs\n",
        "\n",
        "\n",
        "async def scrape_newegg_gpu():\n",
        "    base_url = \"https://www.newegg.com/p/pl?N=100006662&page=\"\n",
        "    all_products = []\n",
        "\n",
        "    async with async_playwright() as p:\n",
        "        browser = await p.chromium.launch(headless=True)\n",
        "        page = await browser.new_page()\n",
        "\n",
        "        # Use a real browser header\n",
        "        await page.set_extra_http_headers({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                          \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n",
        "        })\n",
        "\n",
        "        page.set_default_timeout(60000)\n",
        "\n",
        "        for page_num in range(1, 6):  # scrape first 5 pages\n",
        "            print(f\"🔍 Scraping page {page_num}...\")\n",
        "            url = f\"{base_url}{page_num}\"\n",
        "\n",
        "            try:\n",
        "                await page.goto(url, wait_until=\"domcontentloaded\", timeout=60000)\n",
        "                # Scroll to ensure all items load\n",
        "                for i in range(5):\n",
        "                    await page.mouse.wheel(0, 3000)\n",
        "                    await asyncio.sleep(1)\n",
        "            except PlaywrightTimeoutError:\n",
        "                print(f\"⚠️ Timeout on page {page_num}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            product_elements = await page.query_selector_all(\"div.item-cell\")\n",
        "            if not product_elements:\n",
        "                print(f\"⚠️ No products found on page {page_num}. Stopping.\")\n",
        "                break\n",
        "\n",
        "            for product in product_elements:\n",
        "                try:\n",
        "                    # Name & link\n",
        "                    title_el = await product.query_selector(\"a.item-title\")\n",
        "                    name = await title_el.inner_text() if title_el else None\n",
        "                    link = await title_el.get_attribute(\"href\") if title_el else None\n",
        "\n",
        "                    # Brand\n",
        "                    brand_el = await product.query_selector(\"a.item-brand img\")\n",
        "                    brand = await brand_el.get_attribute(\"title\") if brand_el else None\n",
        "\n",
        "                    # Price\n",
        "                    price_el = await product.query_selector(\"li.price-current\")\n",
        "                    price = await price_el.inner_text() if price_el else None\n",
        "                    if price:\n",
        "                        price = price.replace(\"\\n\", \"\").replace(\"–\", \"\").strip()\n",
        "\n",
        "                    # Shipping info\n",
        "                    ship_el = await product.query_selector(\"div.product-delivery-title\")\n",
        "                    shipping = await ship_el.inner_text() if ship_el else None\n",
        "\n",
        "                    # Extract specs from title\n",
        "                    specs = extract_gpu_specs(name)\n",
        "\n",
        "                    all_products.append({\n",
        "                        \"Name\": name,\n",
        "                        \"Brand\": brand,\n",
        "                        \"GPU Model\": specs.get(\"GPU Model\"),\n",
        "                        \"Chipset Brand\": specs.get(\"Chipset Brand\"),\n",
        "                        \"Memory Size (GB)\": specs.get(\"Memory Size (GB)\"),\n",
        "                        \"Memory Type\": specs.get(\"Memory Type\"),\n",
        "                        \"PCIe Version\": specs.get(\"PCIe Version\"),\n",
        "                        \"Price\": price,\n",
        "                        \"Shipping\": shipping,\n",
        "                        \"Link\": link\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"⚠️ Error reading GPU card: {e}\")\n",
        "                    continue\n",
        "\n",
        "            await asyncio.sleep(2)\n",
        "\n",
        "        await browser.close()\n",
        "\n",
        "    df = pd.DataFrame(all_products)\n",
        "    df.to_csv(\"newegg_gpu.csv\", index=False)\n",
        "    print(f\"✅ Done! {len(df)} GPU products saved to newegg_gpu.csv\")\n",
        "\n",
        "\n",
        "# Run the scraper\n",
        "await scrape_newegg_gpu()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qafD0-OZIMOf",
        "outputId": "8b337b95-930d-423f-ec48-d142e9041fd8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Scraping page 1...\n",
            "🔍 Scraping page 2...\n",
            "⚠️ No products found on page 2. Stopping.\n",
            "✅ Done! 36 GPU products saved to newegg_gpu.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EIfCE7b6KiDO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}